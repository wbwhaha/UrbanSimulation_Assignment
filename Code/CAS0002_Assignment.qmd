---
title: Assessment for CASA0002 – Urban Simulation
subtitle: Network and SIM
bibliography: ../US_wbw_bib/US_wbw.bib
csl: ../harvard-cite-them-right.csl
execute:
  echo: false
  freeze: true
  warning: false
  message: false
format:
  html:
    code-copy: true
    code-link: true
    toc: true
    toc-title: On this page
    toc-depth: 2
    toc_float:
      collapsed: false
      smooth_scroll: true
  pdf:
    code-block: true
    include-in-header:
      
      text: |
        \usepackage{microtype}
        \addtokomafont{disposition}{\rmfamily}
    mainfont: Times New Roman
    sansfont: Times New Roman
    monofont: Times New Roman
    papersize: a4
    geometry:
      - top=25mm
      - left=40mm
      - right=30mm
      - bottom=25mm
      - heightrounded
    toc: false
    number-sections: false
    colorlinks: true
    highlight-style: github
jupyter:
  jupytext:
    text_representation:
      extension: .qmd
      format_name: quarto
      format_version: '1.0'
      jupytext_version: 1.15.2
  kernelspec:
    display_name: Python 3 (ipykernel)
    language: python
    name: python3
---

```{python}
#| include: false

# check the packages
import importlib 
import pip

def check_and_install(package): 
    
    try: importlib.import_module(package) 
    
    except ImportError: 

        print(f"{package} is not installed. Installing...") 
        pip.main(['install', package]) 

# list of required packages 
required_packages = ['pandas', 'numpy', 'geopandas', 'matplotlib', 'mpl_toolkits', 're', 'networkx', 'osmnx', 'geopy'] 
# check and install each package 
for package in required_packages: 
    check_and_install(package)
```

```{python}
# import packages
import pandas as pd
import numpy as np
import geopandas as gpd

import networkx as nx
import osmnx as ox
import re

import matplotlib.pyplot as plt
```

## Part I: London’s underground resilience  
  
## 1. Topological network  
  
### 1.1. Centrality measures:
In the study of transportation networks, particularly complex systems such as the London Underground, it is essential to identify the most structurally and functionally important stations. @bloch_centrality_2023 claimed that centrality measures derived from graph theory provide a rigorous framework for quantifying the relative importance of nodes (stations) within a network. This section introduces and justifies the selection of three centrality measures: *degree centrality*, *betweenness centrality*, and *closeness centrality*, to characterize and identify crucial stations in the underground network.
  
### i Degree Centrality
**Definition and Equation**
Degree centrality is a fundamental measure that captures the number of direct connections a node has within the network. For an undirected graph $G=(V,E)$, where $V$ is the set of nodes and $E$ is the set of edges, the degree centrality $C_D(v)$ of a node $v∈V$ is defined as:
$$ C_D(v) = \frac{\deg(v)}{N - 1} $$
where $\deg(v)$ denotes the degree of node $v$, and $N$ is the total number of nodes in the network. The measure is normalized by $N−1$ to facilitate comparison across networks of different sizes (@derrible_network_2012). 
  
**Application in the Underground Context**
In the context of the London Underground, degree centrality reflects the number of other stations directly accessible from a given station, either through a single train line or by transfer @derrible_network_2012. Stations with high degree centrality, such as King’s Cross St Pancras, typically serve multiple lines or act as interchange points. These stations play a key role in maintaining the local connectivity of the network.
  
**Importance for Network Functioning**
According to @zhang_network_2021, high-degree stations are vital for passenger flow and route flexibility, enabling travelers to access multiple lines without significant detours. Disruption at such nodes may significantly impact service efficiency and passenger distribution, especially during peak hours (@su_simulation-based_2023).
  
### ii Betweenness Centrality
**Definition and Equation**
Betweenness centrality quantifies the extent to which a node lies on the shortest paths between other pairs of nodes. It is formally defined as:
$$ C_B(v) = \sum_{i \ne j \ne v} \frac{\sigma_{ij}(v)}{\sigma_{ij}} $$
where $\sigma_{ij}$ is the total number of shortest paths between nodes $i$ and $j$, and $\sigma_{ij}(v)$ is the number of those paths that pass through node $v$(@freeman_set_1977).
  
**Application in the Underground Context**
Betweenness centrality highlights nodes that serve as transfer corridors or bridging stations within the Underground (@mussone_dynamic_2025). For example, stations such as Green Park or Oxford Circus often lie on the shortest routes connecting disparate parts of the network. These nodes may not have the highest degree but are essential for enabling efficient travel between many station pairs.
  
**Importance for Network Functioning**
@liu_recognition_2018 pinted out that nodes with high betweenness centrality represent potential bottlenecks or points of vulnerability. A failure or congestion at such stations would disrupt many origin-destination pairs, significantly affecting network-wide travel efficiency. Thus, betweenness centrality is instrumental in evaluating the resilience and structural robustness of the Underground.
  
### iii Closeness Centrality
**Definition and Equation**
Closeness centrality measures how near a node is to all other nodes in the network. For a node $v$, it is defined as the inverse of the average shortest path distance to all other nodes:
$$ C_C(v) = \frac{1}{\sum_{v \ne t} d(v, t)} $$
where $d(v,t)$ denotes the length of the shortest path from node $v$ to node $t$, and $N$ is the number of nodes in the network (@bavelas_communication_1950).

**Application in the Underground Context**
Closeness centrality identifies stations that are centrally located in terms of travel time or distance. Stations such as Bank or Holborn are often close to a large number of other stations and can be reached from many locations with relatively few transfers.  
  
**Importance for Network Functioning**
Stations with high closeness centrality enable efficient access to the rest of the network and are critical for minimizing overall travel times. They are often located in geographical and topological centers of the network and serve as ideal points for dispersing or collecting traffic.  
  
### Synthesis and Relevance
Together, these three centrality measures offer complementary insights into the importance of stations in the Underground:  
   - Degree centrality captures local importance and identifies hubs for immediate accessibility and transfers.  
   - Betweenness centrality identifies strategic connectors that hold the network together across routes.  
   - Closeness centrality highlights spatial efficiency, marking stations from which the network is most accessible.
  
By applying these measures in combination, one can systematically identify stations that are structurally and operationally indispensable to the Underground’s performance and resilience. These metrics are crucial not only for understanding current usage patterns but also for planning targeted investments, maintenance priorities, and emergency response strategies (@bavelas_communication_1950).
  
The first 10 ranked nodes for each of the 3 measures are listed below:
```{python}
G_lu = pd.read_pickle('../Geo_Datasets/London_tube_network_cl.pkl')
```

```{python}
deg_london_t =nx.degree_centrality(G_lu)
nx.set_node_attributes(G_lu,dict(deg_london_t),'degree_t')

df_d_t = pd.DataFrame(index=G_lu.nodes())
df_d_t['station_name'] = df_d_t.index
df_d_t['degree_t'] = pd.Series(nx.get_node_attributes(G_lu, 'degree_t'))

df_d_t_sorted = df_d_t.sort_values(["degree_t"], ascending=False)
df_d_t_sorted.reset_index(drop=True, inplace=True)

df_d_t_sorted[0:10]
```

```{python}
bet_london_t=nx.betweenness_centrality(G_lu, normalized=False)
nx.set_node_attributes(G_lu,bet_london_t,'betweenness_t')

df_b_t = pd.DataFrame(index=G_lu.nodes())
df_b_t['station_name'] = df_b_t.index
df_b_t['betweenness_t'] = pd.Series(nx.get_node_attributes(G_lu, 'betweenness_t'))

df_b_t_sorted = df_b_t.sort_values(["betweenness_t"], ascending=False)
df_b_t_sorted.reset_index(drop=True, inplace=True)

df_b_t_sorted[0:10]
```

```{python}
clos_london_t=nx.closeness_centrality(G_lu)
nx.set_node_attributes(G_lu,clos_london_t,'closeness_t')

df_c_t = pd.DataFrame(index=G_lu.nodes())
df_c_t['station_name'] = df_c_t.index
df_c_t['closeness_t'] = pd.Series(nx.get_node_attributes(G_lu, 'closeness_t'))

df_c_t_sorted = df_c_t.sort_values(["closeness_t"], ascending=False)
df_c_t_sorted.reset_index(drop=True, inplace=True)

df_c_t_sorted[0:10]
```
  
### 1.2. Node removal
  
```{python}
G_lu_d=G_lu.copy()
G_lu_b=G_lu.copy()
G_lu_c=G_lu.copy()
```

```{python}
def analyze_resilience(G, centrality_type, num_removals=10):
    """
    Analyze network resilience by sequentially removing the highest-ranked node 
    based on the selected centrality measure and tracking the Largest Connected Component (LCC).

    :param G: NetworkX graph object
    :param centrality_type: Type of centrality ('degree', 'betweenness', 'closeness')
    :param num_removals: Number of nodes to remove (default: 10)
    :return: List of LCC sizes after each removal
    """

    # copy graph to avoid modifying the original
    G = G.copy()
    lcc_sizes = []  # store LCC sizes after each step

    # mapping of centrality functions
    centrality_functions = {
        'degree': nx.degree_centrality,
        'betweenness': nx.betweenness_centrality,
        'closeness': nx.closeness_centrality
    }

    # ensure the chosen centrality type is valid
    if centrality_type not in centrality_functions:
        raise ValueError("Centrality type must be 'degree', 'betweenness', or 'closeness'")

    for i in range(num_removals):
        # compute centrality scores
        centrality = centrality_functions[centrality_type](G)

        # identify the node with the highest centrality score
        node_to_remove = max(centrality, key=centrality.get)

        # remove the selected node from the graph
        G.remove_node(node_to_remove)

        # compute the Largest Connected Component (LCC)
        largest_cc = max(nx.connected_components(G), key=len) if G.number_of_nodes() > 0 else set()
        lcc_size = len(largest_cc)
        lcc_sizes.append(lcc_size)

        print(f"Step {i+1}: Removed {node_to_remove}, LCC size: {lcc_size}")

    # plotting LCC size over node removals
    plt.figure(figsize=(8, 5))
    plt.plot(range(1, num_removals + 1), lcc_sizes, marker='o', linestyle='-')
    plt.title(f'LCC Size vs. Node Removals ({centrality_type.capitalize()} Centrality)')
    plt.xlabel('Number of Nodes Removed')
    plt.ylabel('Size of Largest Connected Component (LCC)')
    plt.grid(True)
    plt.tight_layout()
    plt.show()

    print("Resilience analysis complete.")

    return lcc_sizes
```

To evaluate the structural resilience of the urban rail network, we simulate sequential node removals guided by three classical centrality measures: degree, betweenness, and closeness. After each removal, the Largest Connected Component (LCC) is recalculated to assess the resulting fragmentation. Figure `LCC Size vs. Node Removals by Centrality Measure` presents a comparative line plot of LCC sizes as a function of the number of nodes removed under each centrality-based attack strategy.
  
**Degree centrality**
```{python}
lcc_degree = analyze_resilience(G_lu_d, 'degree')
```

Node removals based on degree centrality cause a slow and steady decline in the size of the largest connected component (LCC), suggesting that highly connected stations are not always structurally critical. The network remains largely intact after several removals, indicating resilience to local failures.
  
**Betweenness centrality**
```{python}
lcc_betweenness = analyze_resilience(G_lu_b, 'betweenness')
```

Betweenness centrality leads to a sharp drop in LCC size after the removal of key bridging nodes. The sixth removal, corresponding to `West Hampstead`, causes fragmentation, showing that betweenness is effective at identifying structural weak points.
  
**Closeness centrality**
```{python}
lcc_closeness = analyze_resilience(G_lu_c, 'closeness')
```

Interestingly, closeness centrality shows a similar pattern, with station `Canada Water` triggering rapid decline. This overlap suggests that closeness can also capture structurally important hubs, especially when spatial and topological centrality coincide.

```{python}
x = list(range(1, len(lcc_degree) + 1))

plt.figure(figsize=(8, 6))

plt.plot(x, lcc_degree, marker='o', label='Degree Centrality')
plt.plot(x, lcc_betweenness, marker='s', label='Betweenness Centrality')
plt.plot(x, lcc_closeness, marker='^', label='Closeness Centrality')

plt.title('LCC Size vs. Node Removals by Centrality Measure')
plt.xlabel('Number of Nodes Removed')
plt.ylabel('Size of Largest Connected Component (LCC)')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()
```

The results reveal that `betweenness centrality` is the most effective measure for identifying structurally critical nodes. The LCC size remains stable through the first five node removals but drops sharply between the fifth and sixth removal, from approximately 375 to 227. This indicates that betweenness-ranked nodes act as bridging stations, whose removal causes major segmentation of the network. Interestingly, `closeness centrality`, while traditionally viewed as a measure of global efficiency rather than structural vulnerability, exhibits a similarly sharp drop. This suggests that some top closeness-ranked nodes may also serve as intermodal connectors with high structural importance.

In contrast, `degree centrality`, which prioritizes nodes with the highest immediate connections—leads to a more gradual reduction in LCC size. The persistence of a large connected component after several degree-based removals indicates that the network topology retains a certain level of redundancy. This echoes observations by @derrible_complexity_2010, who noted that metro systems often embed topological resilience through alternate routes and transfer options.

The sudden inflection in the betweenness and closeness curves suggests that a small subset of nodes holds disproportionate control over network connectivity. These are likely to include transfer hubs such as King’s Cross St. Pancras or Bank, which serve as bottlenecks across multiple lines. This aligns with @freeman_set_1977 foundational theory of betweenness centrality, which emphasized the strategic role of nodes occupying critical positions in the shortest paths of the network. Furthermore, network vulnerability under targeted attacks, as demonstrated in the classic work by @albert_error_2000, is clearly visible here, especially when removals are guided by topological centrality rather than random selection. But the detailed inspection reveals that West Hampstead and Canada Water are the two stations responsible for this dramatic drop in LCC under both betweenness and closeness-based strategies. These stations function as crucial interchanges that connect multiple lines or subnetworks and their removal severs inter-line connectivity, leading to fragmentation. 

Consistent with findings in the transportation literature, such as @freeman_set_1977, our results confirm that betweenness centrality is particularly suited for resilience analysis in transit networks. Its ability to capture inter-line dependencies and chokepoint behavior makes it a valuable tool for identifying high-risk stations. Closeness centrality, although less consistent, may still serve as a complementary indicator in identifying spatially central yet vulnerable nodes.
  
## 2. Flows: weighted network
  
### 2.1. Centrality of weighted network
```{python}
# tlf provides passenger flow data for each network separately
# create a list of all the different files we need
files = ['../Datasets/NBT19MTT2b_od__DLR_tb_wf.csv', 
         '../Datasets/NBT19MTT2b_od__EZL_tb_wf.csv', 
         '../Datasets/NBT19MTT2b_od__LO_tb_wf.csv', 
         '../Datasets/NBT19MTT2b_od__LU_tb_wf.csv']

# create an empty list to store dataframes for each file
dfs = []

# iterate through our files list, read the file and append to our dataframe list
for file in files:
    dfs.append(pd.read_csv(file))
    
# merge all dataframes
london_OD = pd.concat(dfs, ignore_index=True)

definition_df = pd.read_excel('../Datasets/NBT19_Definitions.xlsx', 'Stations')
```

```{python}
london_OD.rename({'1':'Early(3-5)'}, axis=1, inplace=True)
london_OD.rename({'2':'Morning(5-7)'}, axis=1, inplace=True)
london_OD.rename({'3':'AMPeak(7-10)'}, axis=1, inplace=True)
london_OD.rename({'4':'InterPeak(10-16)'}, axis=1, inplace=True)
london_OD.rename({'5':'PMPeak(16-19)'}, axis=1, inplace=True)
london_OD.rename({'6':'Evening(19-22)'}, axis=1, inplace=True)
london_OD.rename({'7':'Late(22-0.5)'}, axis=1, inplace=True)
london_OD.rename({'8':'Night(0.5-3)'}, axis=1, inplace=True)
```

```{python}
# drop any missing values from our dataframe
definition_df.dropna(inplace=True)

# transform mnlc codes from floats to int
definition_df.MNLC = definition_df.MNLC.astype(int)

# add the station names
london_OD['station_origin'] = london_OD['mode_mnlc_o'].apply(lambda x: definition_df[definition_df.MNLC==x]['StationName'].values[0])
london_OD['station_destination'] = london_OD['mode_mnlc_d'].apply(lambda x: definition_df[definition_df.MNLC==x]['StationName'].values[0])

london_OD_sum = london_OD.fillna(0)
```

```{python}
timeband = ['Early(3-5)', 'Morning(5-7)', 'AMPeak(7-10)', 'InterPeak(10-16)', 'PMPeak(16-19)', 'Evening(19-22)', 'Late(22-0.5)', 'Night(0.5-3)']
mode_mnlc = ['mode_mnlc_o',	'mode_mnlc_d']

london_OD_sum['Flow_Sum'] = london_OD_sum[timeband].sum(axis=1)
london_OD_sum.Flow_Sum = london_OD_sum.Flow_Sum.astype(int)

london_OD_sum.drop(columns = timeband, axis=1, inplace=True)
london_OD_sum.drop(columns = mode_mnlc, axis=1, inplace=True)

london_OD_sum
```

```{python}
station_origin_overall = list(london_OD_sum.station_origin.values)
station_destination_overall = list(london_OD_sum.station_destination.values)
all_stations_overall = list(set(station_origin_overall + station_destination_overall))
all_stations_overall = sorted(all_stations_overall, key=str.lower)

london_OD_sum['station_origin']= london_OD_sum.station_origin.apply(lambda x: 
                                      re.sub('\sLU\s?|\sLO\s?|\sNR\s?|\sTf[lL]\s?|\sDLR\s?|\s\(.*\)', '', x)
                                     )
london_OD_sum['station_destination']=london_OD_sum.station_destination.apply(lambda x: 
                                      re.sub('\sLU\s?|\sLO\s?|\sNR\s?|\sTf[lL]\s?|\sDLR\s?|\s\(.*\)', '', x)
                                     )
```

```{python}
london_OD_sum = london_OD_sum.groupby(['station_origin', 'station_destination'], as_index=False).sum()
```

```{python}
OD_names = set(london_OD_sum.station_origin.unique())
network_names = set([n for n in G_lu.nodes()])
```

```{python}
name_map = {
    'Heathrow Terminal 4 EL': 'Heathrow Terminal 4',
    'Heathrow Terminals 123': 'Heathrow Terminals 2 & 3',
    'Heathrow Terminals 2 & 3 EL': 'Heathrow Terminals 2 & 3',
    "Walthamstow Queen's Road": 'Walthamstow Queens Road'
}

london_OD_sum.station_origin = london_OD_sum.station_origin.apply(
    lambda x: name_map[x] if x in name_map.keys() else x
)
london_OD_sum.station_destination = london_OD_sum.station_destination.apply(
    lambda x: name_map[x] if x in name_map.keys() else x
)

OD_names = set(london_OD_sum.station_origin.unique())
_filter = list(network_names.symmetric_difference(OD_names))

london_OD_sum = london_OD_sum[~london_OD_sum.station_origin.isin(_filter)]
london_OD_sum = london_OD_sum[~london_OD_sum.station_destination.isin(_filter)]

# create a dictionary to store flows for all edges
flows = {(u,v): 0 for u,v in G_lu.edges()}

# calculate shortest paths for all flows and add data to dict
for i, row in london_OD_sum.iterrows():
    source = row.station_origin
    target = row.station_destination
    
    # get shortest path
    path = nx.dijkstra_path(G_lu, source, target)
    
    # path is a list of nodes, need to be turned to a list of edges
    path_edges = list(zip(path,path[1:])) 
    
    # add flows to our dict
    for u,v in path_edges:
        try:
            flows[(u,v)] += row.Flow_Sum
        except:
            flows[(v,u)] += row.Flow_Sum
```

```{python}
# set this as a network attribute
nx.set_edge_attributes(G_lu, flows, 'Flow_Sum')
```

The most important station according to each these 3 measures in the weighted network and the computing process:
  
Degree Centrality:
```{python}
#| echo: true
deg_london_w = dict(G_lu.degree(weight='Flow_Sum'))

nx.set_node_attributes(G_lu,deg_london_w,'degree_w')

max_node_d = max(deg_london_w, key=deg_london_w.get)

print(f"The node with the highest degree centrality is: {max_node_d},\n" 
      f"with a value of: {deg_london_w[max_node_d]}")
```
Betweenness centrality:
```{python}
#| echo: true
bet_london_w=nx.betweenness_centrality(G_lu,weight='Flow_Sum',normalized=False)

nx.set_node_attributes(G_lu,bet_london_w,'betweenness_w')

max_node_b = max(bet_london_w, key=bet_london_w.get)

print(f"The node with the highest betweenness centrality is: {max_node_b},\n" 
      f"with a value of: {bet_london_w[max_node_b]}")
```
Closeness centrality:
```{python}
#| echo: true
clos_london_w=nx.closeness_centrality(G_lu, distance='Flow_Sum')

nx.set_node_attributes(G_lu,clos_london_w,'closeness_w')

max_node_c = max(clos_london_w, key=clos_london_w.get)

print(f"The node with the highest closeness centrality is: {max_node_c},\n" 
      f"with a value of: {clos_london_w[max_node_c]}")
```

```{python}
# create Origin Destination matrix
OD = pd.pivot_table(london_OD_sum, 
                    values ="Flow_Sum", 
                    index="station_origin", 
                    columns = "station_destination",
                    aggfunc=sum, 
                    margins=True)
OD.fillna(0, inplace=True)
OD = OD.astype(int)
```

```{python}
# get total at origin and total at destinations
origin = OD.loc[:,'All'].to_dict()
destination = OD.loc['All',:].to_dict()
```

```{python}
# add this data as population
london_OD_sum['population'] = london_OD_sum.station_origin.apply(lambda x: origin[x])
```

### 2.2. Largest OD flow
```{python}
london_OD_sum.sort_values('Flow_Sum',ascending=False).head(10)
```

```{python}
# get edge with highest number of flows
print(f"The largest OD flow is {max(flows, key=flows.get)}")
```

```{python}
Flow_affected = london_OD_sum.loc[london_OD_sum['station_origin'] == 'Bank and Monument', 'Flow_Sum'].sum()

print(f"Number of people affected: {Flow_affected}")
```

```{python}
from pyproj import Transformer

# create a coordinate transformer from EPSG:27700 (British National Grid) to EPSG:4326 (WGS 84 - latitude/longitude)
transformer = Transformer.from_crs("EPSG:27700", "EPSG:4326", always_xy=True)

# iterate through each node in the graph
for node in G_lu.nodes:
    # extract x and y coordinates from node attributes
    x, y = G_lu.nodes[node]['coords']
    
    # convert coordinates to longitude and latitude
    lon, lat = transformer.transform(x, y)
    
    # update node attributes with transformed coordinates
    G_lu.nodes[node]['coords'] = (lon, lat)
```

### 2.3. Optimal alternatives and Walking time analysis
In order to find the nearest station, we write code to identify alternative underground stations within walking distance (1 km) of the closed "Bank and Monument" station. It uses the geodesic distance to filter nearby stations and then downloads the local pedestrian street network via OSMnx. For each candidate station, it calculates the shortest walking route using the road network and estimates walking time based on a speed of `1.2 m/s` (**based on UK Department for Transport – WebTAG guidance, Similarly, we also use this speed as the standard in the analysis of the second part.**). The results, including station name, path length, and walking time, are compiled into a sorted DataFrame, which is shown below.
```{python}
from geopy.distance import geodesic

# get coordinates of the closed origin station
origin_name = 'Bank and Monument'
lon1, lat1 = G_lu.nodes[origin_name]['coords']
origin_coords = (lat1, lon1)  # geopy uses (lat, lon)

# download pedestrian road network
G_walk = ox.graph_from_point(origin_coords, dist=1000, network_type='walk')

# get nearest node in road network for origin station
origin_node = ox.distance.nearest_nodes(G_walk, lon1, lat1)

# create list of candidate alternative stations
alt_stations = {
    node: data['coords'] for node, data in G_lu.nodes(data=True) if node != origin_name
}

# initialize a list to store results
results = []

# filter by straight-line distance, then compute walking time
for name, (lon2, lat2) in alt_stations.items():
    dest_coords = (lat2, lon2)
    
    crow_distance = geodesic(origin_coords, dest_coords).meters
    if crow_distance > 1000:
        continue

    try:
        dest_node = ox.distance.nearest_nodes(G_walk, lon2, lat2)
        path_length = nx.shortest_path_length(G_walk, origin_node, dest_node, weight='length')
        walk_time_min = path_length / 72  # walking speed: 1.2 m/s (72 m/min)

        # append result as a dictionary
        results.append({
            'Station': name,
            'PathLength_m': round(path_length),
            'WalkTime_min': round(walk_time_min, 1)
        })

    except Exception as e:
        # log error if needed
        continue

# convert list to DataFrame
df_walk_alternatives = pd.DataFrame(results)

# sort by walking time
df_walk_alternatives = df_walk_alternatives.sort_values(by='WalkTime_min')

# display the DataFrame
df_walk_alternatives
```

Thus, they should go to `Canno Street` station as an alternative, whcih will take them about `5.6` minutes, and the shorest path is shown below.

```{python}
# get the station with shortest walk time
best_station = df_walk_alternatives.iloc[0]['Station']
lon_best, lat_best = G_lu.nodes[best_station]['coords']

# find nearest nodes in walking network
dest_node = ox.distance.nearest_nodes(G_walk, lon_best, lat_best)

# reuse origin node
origin_node = ox.distance.nearest_nodes(G_walk, lon1, lat1)

# get shortest path
route = nx.shortest_path(G_walk, origin_node, dest_node, weight='length')

# plot the route
fig, ax = ox.plot_graph_route(G_walk, route, node_size=2)

ax.legend()

plt.show()
```

{{< pagebreak >}}

## Part II: Spatial Interaction models
  
## 1. Models and calibration
  
### 1.1. Model introduction
Spatial interaction models are mathematical frameworks used to estimate flows between origins and destinations, factoring in both the attractiveness of destinations and the deterrent effect of distance or travel cost. Among these, urban gravity models specifically focus on predicting and analyzing the movement of people, goods, and services within urban and regional systems, applying principles similar to gravitational forces to explain spatial flows. Inspired by Newton's law of gravitation, these models assume that interaction between two locations is directly proportional to their "masses"， such as population, employment and amenities, and inversely proportional to some function of the distance or cost between them (@wilson_family_1971). Here, we introduce four commonly used gravity model variants: the *unconstrained*, *origin-constrained*, *destination-constrained*, and *doubly constrained models*.
  
### i Unconstrained Gravity Model
The unconstrained gravity model estimates flows between two locations $i$ and $j$ as:
$$ T_{ij} = k \cdot \frac{O_i^\alpha D_j^\gamma}{f(c_{ij})} $$
where:
  - $T_{ij}:$ predicted flow from origin $i$ to destination $j$
  - $O_i:$ origin mass (e.g., population at $i$)
  - $D_j:$ destination mass (e.g., jobs or services at $j$)
  - $f(c_{ij}):$ deterrence function, typically increasing with cost $c_{ij}$ (e.g., travel time or distance)
  - $k:$ normalization constant
  - $\alpha:$ Exponent on the origin variable — adjusts how strongly origin capacity influences flow
  - $\gamma:$ Exponent on the destination variable — adjusts how strongly destination attractiveness influences flow
A common choice for $f(c_{ij})$ is an exponential or power function, such as:
$$ f(c_{ij}) = c_{ij}^{\beta} \quad \text{or} \quad f(c_{ij}) = e^{-\beta c_{ij}} $$
where $\beta>0$ is a deterrence parameter that governs sensitivity to distance.
  
In the practicals and assignment, we choose $f(c_{ij}) = c_{ij}^{\beta}$, and cost factor is often represented by distance $d$. So,  we can get $f(c_{ij}) = d_{ij}^{\beta}$
  
**Use Case - Exploratory Urban Interaction Analysis**
The unconstrained gravity model is particularly useful in exploratory analyses where no prior knowledge of total flows from origins or to destinations is available. It is commonly applied in early-stage urban studies or simulations to infer potential spatial interaction patterns based solely on population size and accessibility. For instance, it can be used to estimate hypothetical trade volumes or migration intensities between cities in the absence of empirical flow data, assuming that larger cities attract more flows and that distance acts as a frictional constraint. This approach has also been used in cross-national contexts, such as modeling international migration or tourism flows without border-specific data, suggested by @liu_uncovering_2014. However, the lack of constraints may limit its predictive accuracy in practical planning scenarios.
  
### ii Origin-Constrained Gravity Model
This model constrains flows to match known total outflows from each origin:
$$ T_{ij} = O_i \cdot \frac{D_j^\gamma f(c_{ij})}{\sum_j D_j^\gamma f(c_{ij})} \quad (\alpha=1) $$
where
$$ O_i = \sum_j T_{ij} $$
Here, $O_i$ is fixed, and flows are allocated among destinations $j$ based on their attractiveness $D_j$ and proximity to $i$. The denominator ensures that the total outflow from each $i$ matches $O_i$.
thus
$$ A_i = \frac{1}{\sum_j D_j^\gamma f(c_{ij})} $$
  
**Use Case - Commuting and Travel Demand Estimation**
The origin-constrained model is most suitable when total flows from each origin like residential zone are known but their distribution among destinations is unknown. A prominent application is in commuting flow estimation, where the number of workers living in each neighborhood is given, but their specific job locations must be inferred based on job densities and travel costs. Transportation planners frequently employ this model to forecast travel demand from suburbs into urban cores, particularly in developing cities where detailed employment destination data may be lacking. It has also been adopted in health accessibility studies to evaluate patient flows from homes to medical facilities while holding origin population fixed (@wilson_family_1971).
  
### iii Destination-Constrained Gravity Model
This model is symmetrical to the origin-constrained version, ensuring the total inflows to each destination are fixed:
$$ T_{ij} = D_j \cdot \frac{O_i^\alpha f(c_{ij})}{\sum_i O_i f(c_{ij})} \quad (\gamma=1) $$
where
$$ D_j = \sum_i T_{ij} $$
which is exactly the opposite of the previous one. $D_j$ is fixed, and flows are allocated among origins $i$ based on their features $O_i$ and proximity to $j$. The denominator ensures that the total outflow from each $j$ matches $D_j$.
thus
$$ B_j = \frac{1}{\sum_i O_i^\alpha f(c_{ij})} $$
  
**Use Case - Facility Access or Service Catchment Modeling**
When destination capacities or inflow totals are fixed—such as the number of students a school can accept or the maximum number of patients a hospital can serve—the destination-constrained gravity model is appropriate. This model allocates demand across origins based on their relative proximity and population size while ensuring that no destination is overloaded. As indicated by @wills_flexible_1986, it is commonly used in health services planning, disaster response logistics, and educational catchment modelling. For example, a regional health department may use this model to estimate how many patients each hospital will receive from different neighborhoods, assuming each hospital has a capped intake capacity and patients prefer closer facilities.

### iv Doubly Constrained Gravity Model
This model imposes constraints on both origins and destinations:
$$ T_{ij} = A_i B_j O_i D_j f(c_{ij}) \quad (\alpha=\gamma=1) $$ 
with balancing factors $A_i$ and $B_j$ ensuring the row and column totals match the known origin and destination flows:
$$ A_i = \frac{1}{\sum_j B_j D_j f(c_{ij})}\quad and \quad B_j = \frac{1}{\sum_i A_i O_i f(c_{ij})} $$
This formulation is usually solved iteratively (Furness method) and produces the most accurate fit when both marginals are known.

**Use Case - Transport and Urban Systems Calibration**
The doubly constrained gravity model is the most data-intensive but also the most accurate among the gravity model family, making it ideal for calibrated modeling in well-studied urban systems. It is used when both the total outflows from each origin and the total inflows to each destination are known, such as in household travel surveys or census-based commuting datasets. This model is widely applied in regional transportation models, logistics routing, and land-use forecasting, particularly in metropolitan areas with reliable OD (origin-destination) matrices. For example, metropolitan planning organizations (MPOs) use this model to simulate peak-hour transit flows between neighborhoods and employment hubs, taking into account actual boarding and alighting totals (@boyce_forecasting_2015). Its iterative balancing procedure ensures that predicted flows match empirical margins closely, making it a standard in transport modeling software.
  
### 1.2. Model selection and calibration
The gravity model posits that the interaction $T_{ij}$ between origin $i$ (population location, in our case, represented by the centroid of OAs) and destination $j$ (supermarket) increases with:
  - the attractiveness of the destination (supermarket area), and
  - the size of the origin (population),
and decreases with the cost of travel (distance).
  
All the features make it ideal for modeling accessibility and shopping behavior, where people are drawn more to larger supermarkets but are deterred by greater distance. Since no actual flow data (e.g., shopper counts) are available, a constrained gravity model can be used. Origin-constrained model is most suitable, because the total number of trips from each origin is assumed to be equal to the population. Furthermore, through the $\gamma$ and $\beta$ values ​​that have been calibrated in practical, we directly substitute the known parameters into the model and can directly calculate $A_i$ of each origin. Finally, we could get $T_{ij}$.


```{python}
# grab the street network for our area of interest
G = ox.graph.graph_from_place('Borough of Havering, London, UK', network_type='walk')

# get the administrative boundary of the area
area = ox.geocode_to_gdf('Borough of Havering, London, UK')
```

```{python}
retail = pd.read_csv("../Datasets/geolytix_retailpoints_v34_202412.csv")
retail = gpd.GeoDataFrame(retail, geometry=gpd.points_from_xy(retail['long_wgs'], retail['lat_wgs']), crs="EPSG:4326")
retail = retail[retail['geometry'].within(area.loc[0, 'geometry'])]

def extract_square_meter(x):
    # match the square meter value, even if there is a range
    match = re.search(r'\((\d{1,3}(?:,\d{3})*)\s*(?:<\s*\d{1,3}(?:,\d{3})*)?\s*m2\)', x)
    if match:
        # extract the first value in the range, or the single value
        square_meter_str = match.group(1).replace(",", "")
        return int(square_meter_str)
    return None

retail['square_meters'] = retail['size_band'].apply(lambda x: extract_square_meter(x))
```

```{python}
nodes, edges = ox.graph_to_gdfs(G)
```

```{python}
retail['nodes'] = ox.nearest_nodes(G, retail['long_wgs'], retail['lat_wgs'])
```

```{python}
# get the administrative units: OAs and take only the area defined
oa = gpd.read_file("../Geo_Datasets/loac_OA.gpkg")
oa = oa.to_crs(4326)
oa = oa[oa['geometry'].centroid.within(area.loc[0, 'geometry'])]
```

```{python}
# plot the nodes represent each supermarkets
fig, ax = plt.subplots(figsize=(10, 10), facecolor='k')

oa.plot(ax=ax, facecolor='w', alpha=0.2,edgecolor='violet', linewidth=1)

edges.plot(color='w', ax=ax, linewidth=0.3, alpha=0.5)

retail.plot(color='cyan', ax=ax, markersize=20)
retail.plot(color='w', ax=ax, markersize='square_meters', alpha=0.5)

ax.set_facecolor('k')
```

```{python}
oa_ids = ['E00011326', 'E00011710']  # get the actual OA IDs of potienal supermarket
selected_oas = oa[oa['OA21CD'].isin(oa_ids)]
```

```{python}
# take the centroid of each area and compute the distance
oa['lon'] = oa['geometry'].centroid.x
oa['lat'] = oa['geometry'].centroid.y

# join the centroids to its closest node
oa['nodes'] = ox.nearest_nodes(G, oa['lon'], oa['lat'])
```

```{python}
selected_oas_ = oa[oa['OA21CD'].isin(oa_ids)]

selected_nodes_ids=selected_oas_['nodes'].values
```

```{python}
# get origin and destination node lists

# origin nodes from all OA centroids
origin_nodes = oa['nodes'].values

# combine all destination nodes: retail + the 2 selected OAs
destination_nodes = list(retail['nodes'].values) + list(selected_nodes_ids)
```

```{python}
# compute the distance matrix

# create an empty distance matrix
distance_matrix = pd.DataFrame(index=origin_nodes, columns=destination_nodes)

# compute shortest path lengths
for origin in origin_nodes:
    lengths = nx.single_source_dijkstra_path_length(G, origin, weight='length')
    for dest in destination_nodes:
        distance_matrix.loc[origin, dest] = lengths.get(dest, float('inf'))  # 'inf' if no path
```

```{python}
# add OA21CD as index labels
distance_matrix.index = oa['OA21CD'].values

# label destination columns: retail + selected OAs
retail_labels = retail['id'].astype(str).values  # ensures strings
selected_labels = oa_ids
distance_matrix.columns = list(retail_labels) + selected_labels
```

```{python}
#| echo: true
distance_matrix.loc['E00011326', 'E00011326'] = 1
distance_matrix.loc['E00011710', 'E00011710'] = 1
```
For the two potential areas where supermarkets may be added, the coordinates of the origin and the proposed supermarket location coincide. This results in zero values in the distance matrix, which may cause computational issues. To prevent this, all zero distances are replaced with 1.
```{python}
df_model = distance_matrix.stack().reset_index()
df_model.columns = ['Origin', 'Destination', 'Distance']
```

```{python}
pop = pd.read_csv("../Datasets/census2021-ts001-oa.csv")
pop_sub = pop.loc[:, ['geography code', 'Residence type: Total; measures: Value']]
```

```{python}
df_model = pd.merge(df_model, pop_sub, left_on='Origin', right_on='geography code', how='left')
df_model.drop(columns='geography code', inplace=True)
df_model.rename({'Residence type: Total; measures: Value': 'Population'}, axis=1, inplace=True)
```

```{python}
retail_sub = retail.loc[:, ['id', 'square_meters']]
retail_sub['id'] = retail_sub['id'].astype(str)
```

```{python}
df_model = pd.merge(df_model, retail_sub, left_on='Destination', right_on='id', how='left')
df_model.drop(columns='id', inplace=True)
df_model.rename({'square_meters':'Area'}, axis=1, inplace=True)
```

```{python}
#| echo: true
Average_Area = retail_sub['square_meters'].mean()

df_model.loc[df_model['Destination'] == 'E00011326', 'Area'] = Average_Area
df_model.loc[df_model['Destination'] == 'E00011710', 'Area'] = Average_Area
```
Assume that the size of both new supermarkets is the same and corresponds to the mean of all sizes.
```{python}
df_model_transport_increase = df_model.copy()
```

```{python}
df_model_sub = df_model[(df_model['Destination'] != 'E00011326') & (df_model['Destination'] != 'E00011710')]
df_model_E00011326 = df_model[df_model['Destination'] != 'E00011710']
df_model_E00011710 = df_model[df_model['Destination'] != 'E00011326']

df_model_E00011326_ = df_model_E00011326.copy()
df_model_E00011710_ = df_model_E00011710.copy()
```

```{python}
def Production_Constrained_Model(df, gamma=2.0440, beta=2.2140):
    """
    Computes trip distribution using a gravity model.

    Parameters:
    df (pd.DataFrame): Input dataframe containing 'Origin', 'Destination', 'Distance', 'Area', and 'Population'.
    gamma (float): Sensitivity to attraction (default 2.0440).
    beta (float): Sensitivity to distance (default 2.2140).

    Returns:
    pd.DataFrame: Updated dataframe with computed T_ij values.
    """
    
    # avoid division by zero for any zero distances
    df['Distance'] = df['Distance'].replace(0, 1)

    # compute the attraction term: D_j^gamma * d_ij^-beta
    df['attraction_term'] = (df['Area'] ** gamma) * (df['Distance'] ** -beta)

    # group by Origin and compute the sum of attraction terms for each origin
    Ai_df = df.groupby('Origin')['attraction_term'].sum().reset_index()

    # calculate A_i as the inverse of the sum of attraction terms
    Ai_df['A_i'] = 1 / Ai_df['attraction_term']
    Ai_df = Ai_df[['Origin', 'A_i']]  # Keep only needed columns

    # merge A_i back into the original dataframe
    df = df.merge(Ai_df, on='Origin', how='left')

    # compute T_ij: the flow from origin i to destination j
    df['T_ij'] = df['A_i'] * df['Population'] * (df['Area'] ** gamma) * (df['Distance'] ** -beta)

    # check if the sum of flows from each origin equals the origin's population
    check = df.groupby('Origin')['T_ij'].sum().reset_index()
    check = check.merge(df[['Origin', 'Population']].drop_duplicates(), on='Origin')
    check['difference'] = check['T_ij'] - check['Population']

    return df, check
```

```{python}
result_basic = Production_Constrained_Model(df_model_sub)

df_model_sub = result_basic[0]
check_basic = result_basic[1]
```

```{python}
check_basic
```
For each origin, the total estimated flow to all destinations matches exactly the total flow, which means $\sum_j T_{ij} = O_i$.
## 2. Scenarios
  
### 2.1. Location of new supermarket
The same model and method as in the previous section are applied here, with the only difference being the distance matrix used. In the following analysis, potential supermarket investment locations will be incorporated into the distance matrix to recalculate $A_i$  respectively, and predict $T_{ij}$ subsequently.
​  
Check the origin constrained condition.
  
```{python}
result_E00011326 = Production_Constrained_Model(df_model_E00011326)

df_model_E00011326 = result_E00011326[0]
check_E00011326 = result_E00011326[1]
```
For E00011326:
```{python}
check_E00011326
```

```{python}
result_E00011710 = Production_Constrained_Model(df_model_E00011710)

df_model_E00011710 = result_E00011710[0]
check_E00011710 = result_E00011710[1]
```
For E00011710:
```{python}
check_E00011710
```
Finally, we compare the total flow $\sum T_{ij}$ reach these two potential loactions and get the result:
```{python}
# calculate potential customer flow for two supermarket locations
Flow_E00011326 = round(df_model_E00011326.loc[df_model_E00011326['Destination'] == 'E00011326', 'T_ij'].sum())
Flow_E00011710 = round(df_model_E00011710.loc[df_model_E00011710['Destination'] == 'E00011710', 'T_ij'].sum())

# print all results first
print(f"Estimated customer flow for E00011326: {Flow_E00011326}")
print(f"Estimated customer flow for E00011710: {Flow_E00011710}")

# compare and determine the better location
if Flow_E00011326 > Flow_E00011710:
    print("Location E00011326 is better, with a higher estimated customer count.")
elif Flow_E00011710 > Flow_E00011326:
    print("Location E00011710 is better, with a higher estimated customer count.")
else:
    print("Both locations have the same estimated customer count.")
```
  
### 2.2. Transport increasement
```{python}
def simulate_tij_vs_beta(dfs_with_labels, gamma, beta_range, reference_beta=None):
    """
    Simulate and plot total T_ij received by selected destinations using their full OD data.

    Parameters:
    - dfs_with_labels: list of tuples [(dest_label, df)], where each df includes full OD data 
      but we only extract T_ij for dest_label
    - gamma: exponent for attractiveness (Area^gamma)
    - beta_range: list or array of beta values to simulate
    - reference_beta: optional beta value to mark with vertical line
    """
    import matplotlib.pyplot as plt
    results = {}

    for dest_label, df in dfs_with_labels:
        tij_list = []

        for beta in beta_range:
            df['attraction'] = (df['Area'] ** gamma) * (df['Distance'] ** -beta)
            df['Ai'] = 1 / df.groupby('Origin')['attraction'].transform('sum')
            df['T_ij'] = df['Ai'] * df['Population'] * df['attraction']

            # Only sum flows to the specified destination
            total_flow = df[df['Destination'] == dest_label]['T_ij'].sum()
            tij_list.append(total_flow)

        results[dest_label] = tij_list

    # Plotting
    plt.figure(figsize=(10, 6))
    for dest_label, tij_list in results.items():
        plt.plot(beta_range, tij_list, label=f"To {dest_label}", marker='o')

    if reference_beta:
        ymax = max(max(vals) for vals in results.values())
        plt.axvline(x=reference_beta, color='gray', linestyle=':', linewidth=2)
        plt.text(reference_beta + 0.02, ymax * 0.95, f'β = {reference_beta}', color='gray')

    plt.xlabel("Beta (distance decay parameter)")
    plt.ylabel("Total T_ij (to destination)")
    plt.title("Impact of Beta on T_ij to Selected Destinations")
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    plt.show()
```

```{python}
simulate_tij_vs_beta(dfs_with_labels=[('E00011326', df_model_E00011326_),  ('E00011710', df_model_E00011710_)],
                     gamma=2.0440, 
                     beta_range=np.linspace(0.5, 5.0, 300), 
                     reference_beta=2.2140)
```
The parameter $\beta$ in the gravity model captures how sensitive people are to distance when deciding where to travel—commonly referred to as the distance decay effect. A higher β means that people are more deterred by distance, so closer destinations attract disproportionately more flow. In contrast, a lower β implies weaker distance deterrence, allowing farther destinations to compete more equally (@liu_understanding_2012). In the graph, we observe that as β increases, total flows to both E00011326 and E00011710 rise, but the relative advantage shifts: E00011326 dominates at lower β values, while E00011710 overtakes it at higher β, suggesting the latter benefits more when local flows are emphasized. Assuming a sharp increase in transport efficiency, for instance, better connectivity or reduced travel time, the effective $\beta$ would decrease, reducing the penalty of distance. Under this shift, E00011326 would likely regain its dominance, as more distant flows become viable and its broader appeal can outweigh proximity advantages.
  
On the ther hand, When $\beta$ increases, the gravity model applies a stronger penalty to distance, meaning people become more likely to choose nearby destinations and less likely to travel far. In practical terms, as β grows:  
   - Flows become more localized: distant destinations receive less flow, while nearby destinations gain a larger share.  
   - The influence of destination attractiveness (e.g., size or services) is suppressed by distance.  
   - The spatial interaction pattern becomes more fragmented, favoring dense clusters or locally dominant areas.  
  
In the graph, as $\beta$ increases, both destinations receive more flow—but E00011710 eventually surpasses E00011326 at $\beta≈3.8$. This suggests that E00011710 is more locally accessible or surrounded by denser origins, and thus benefits more in a high-β scenario. Conversely, E00011326 likely draws from a broader spatial base, which gets suppressed as β rises.
  
If $\beta$ continues to grow, the system may converge toward a pattern where only the closest and most immediate destinations dominate, regardless of their absolute attractiveness—reflecting a highly frictional spatial system with limited mobility.

### 2.3. Extra - 15 min Circle
```{python}
oa = pd.merge(oa, pop_sub, left_on='OA21CD', right_on='geography code', how='left')
oa.drop(columns='geography code', inplace=True)
oa.rename({'Residence type: Total; measures: Value': 'Population'}, axis=1, inplace=True)
```
We evaluate pedestrian accessibility to retail locations within a 15-minute walking distance for each Output Area (OA). Assuming a walking speed of **72** meters per minute, we define a walk radius of 1,080 meters. For each OA, we use a street network graph ($G$) to generate an ego subgraph representing all nodes reachable within that distance. We Then check whether any retail locations, identified by their `osmid`, fall within this reachable area. The results are compiled into a DataFrame indicating whether each OA has access to at least one retail node, along with its population. Finally, we summarize the total number of OAs, those with access, and the covered and uncovered populations:
```{python}
# set walking parameters
walk_speed_m_per_min = 72
time_limit_min = 15
walk_radius = walk_speed_m_per_min * time_limit_min 

# build lookup set of all retail osmid
retail_nodes = set(retail['nodes'])

# for each OA node, check if any retail node is reachable within 1250m
accessible_oa = []

for _, row in oa.iterrows():
    oa_node = row['nodes']
    population = row['Population']
    oa_id = row.get('OA21CD', None)

    # build subgraph of all nodes reachable from this OA node within walk_radius
    try:
        subgraph = nx.ego_graph(G, oa_node, radius=walk_radius, distance='length')
        reachable_nodes = set(subgraph.nodes)

        # check if any retail node is within reachable nodes
        if reachable_nodes & retail_nodes:
            accessible_oa.append({'OA21CD': oa_id, 'population': population, 'accessible': True})
        else:
            accessible_oa.append({'OA21CD': oa_id, 'population': population, 'accessible': False})
    except:
        # if the node doesn't exist in G (edge case)
        accessible_oa.append({'OA21CD': oa_id, 'population': population, 'accessible': False})

# convert to DataFrame and summarize
access_df = pd.DataFrame(accessible_oa)

summary = {
    "Total OAs": len(access_df),
    "OAs with access to ≥1 supermarket": access_df['accessible'].sum(),
    "Total population": access_df['population'].sum(),
    "Covered population": access_df.loc[access_df['accessible'], 'population'].sum(),
    "Uncovered population": access_df.loc[~access_df['accessible'], 'population'].sum()
}

summary = pd.DataFrame.from_dict(summary, orient='index', columns=['Value'])

summary
```

```{python}
oa_15min = pd.merge(oa, access_df, on='OA21CD', how='left')

oa_15min = oa_15min[oa_15min['accessible'] == True]
```
The OAs that can walk to a supermarket within 15 minutes is shown in the figure below.
```{python}
fig, ax = plt.subplots(figsize=(10, 10), facecolor='k')

# plot all OAs
oa.plot(ax=ax, facecolor='w', alpha=0.2, edgecolor='violet', linewidth=1)

# plot roads: edges
edges.plot(ax=ax, color='w', linewidth=0.3, alpha=0.5)

# plot selected OAs which could reach a supermarket within 15 min — highlighted in green
oa_15min.plot(ax=ax, facecolor='green', edgecolor='red', linewidth=0.01)

# set background
ax.set_facecolor('k')
ax.set_title("OAs where supermarkets can be reached within 15 min", color='white')

# turn off axis
ax.axis('off')

plt.show()
```
  
  
  
  
Due to space limitations, some parts of the code have been omitted, or may not be fully rendered in the PDF. The complete code can be found [here](https://github.com/wbwhaha/UrbanSimulation_Assignment)(in the 'Code' folder, including all the qmd, ipynb and pdf files).

{{< pagebreak >}}

## References